{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before beginning, I did research on how best to deal with the incredibly imbalanced dataset and different sampling methods.\n",
    "# I initially did my modeling in RapidMiner, but found that I wasn't getting good results that generalized well when I submitted\n",
    "# to Kaggle. I also didn't think the algorithms I was using were doing that well.\n",
    "# I also did research on which models are best for this kind of data, and came across some boosting algorithms that would work\n",
    "# efficiently with high performance generalization, so I switched to Python. I chose to try to optimize various versions of \n",
    "# XGBoost, CatBoost, and LightGBM, and tried various ensembling methods. I found that my best results came from an ensemble\n",
    "# of my best of each of the three types of models, using a threshold of 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the following resources/tutorials about parameter optimization and XGBoost, CatBoost, and LightGBM:\n",
    "# https://github.com/catboost/tutorials/blob/master/classification/classification_with_parameter_tuning_tutorial.ipynb\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\n",
    "# https://github.com/catboost/catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages & modules\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, classification_report, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.utils import shuffle\n",
    "import lightgbm as lgb\n",
    "import catboost as cgb\n",
    "import catboost.datasets as cbd\n",
    "import catboost.utils as cbu\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "import hyperopt\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seet seed - my favorite number, obviously\n",
    "seed = 5\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training dataset\n",
    "data = pd.read_csv('HW4 - training data.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train instances: 5390\n",
      "Number of test instances: 2310\n"
     ]
    }
   ],
   "source": [
    "# sampling - I used undersampling of the majority class and found this to work\n",
    "# better than oversampling the minority. I tested multiple ratios, but found that \n",
    "# a 1:4 ratio worked best for all of my models, using all of the minority class.\n",
    "\n",
    "minority_class = np.array(data[data.adopter == 1].index)\n",
    "majority_class = data[data.adopter == 0].index\n",
    "\n",
    "# a 1:4 ratio using all of the minority means 1540 minority and 6160 majority\n",
    "# let's randomly select the 6160 majority samples\n",
    "# I did this without replacement to add more variety to the samples\n",
    "random_majority_class = np.random.choice(majority_class, 6160, replace = False)\n",
    "random_majority_class = np.array(random_majority_class)\n",
    "\n",
    "# combine both classes now\n",
    "sample = np.concatenate([minority_class, random_majority_class])\n",
    "\n",
    "# get the data using the sample indices and shuffle the data before split\n",
    "sample_data = data.iloc[sample,:]\n",
    "sample_data = shuffle(sample_data)\n",
    "\n",
    "# split into X and y\n",
    "X = sample_data.iloc[:, data.columns != 'adopter']\n",
    "y = sample_data.iloc[:, data.columns == 'adopter']\n",
    "\n",
    "# split dataset for cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print (\"Number of train instances: {}\".format(len(X_train)))\n",
    "print (\"Number of test instances: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this tutorial as a resource for tuning my parameters for LightGBM using Bayesian Optimization:\n",
    "# https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and test into a lightgbm Dataset\n",
    "train_data = lgb.Dataset(X_train, label = y_train)\n",
    "test_data = lgb.Dataset(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objective function and parameters\n",
    "def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight):\n",
    "        params = {'application':'regression','num_iterations': 5000,\n",
    "                  'learning_rate':0.05, 'early_stopping_round':50,\n",
    "                  'metric':'l1'} # Default parameters\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=5, seed=7, stratified=False, verbose_eval =None, metrics=['l1'])\n",
    "        return -np.min(cv_result['l1-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters to search through\n",
    "pds = {'num_leaves': (45, 60),\n",
    "          'feature_fraction': (0.1, 0.9),\n",
    "          'bagging_fraction': (0.8, 1),\n",
    "          'max_depth': (9, 13 ),\n",
    "          'min_split_gain': (0.001, 0.1),\n",
    "          'min_child_weight': (30, 50)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_sp... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "|  1        | -0.255    |  0.8153   |  0.7239   |  10.75    |  44.47    |  0.09782  |  53.08    |\n",
      "|  2        | -0.2651   |  0.9002   |  0.1576   |  10.07    |  40.0     |  0.06824  |  57.06    |\n",
      "|  3        | -0.2649   |  0.8762   |  0.1527   |  10.15    |  48.19    |  0.02213  |  51.78    |\n",
      "|  4        | -0.2678   |  0.9862   |  0.1199   |  11.4     |  49.0     |  0.0238   |  53.23    |\n",
      "|  5        | -0.2644   |  0.9818   |  0.2065   |  11.09    |  45.01    |  0.06723  |  52.02    |\n",
      "|  6        | -0.2525   |  0.8      |  0.9      |  13.0     |  30.0     |  0.001    |  45.0     |\n",
      "|  7        | -0.2543   |  0.8115   |  0.8521   |  9.219    |  30.34    |  0.04937  |  59.59    |\n",
      "|  8        | -0.2535   |  0.8138   |  0.7768   |  9.049    |  30.96    |  0.05473  |  45.16    |\n",
      "|  9        | -0.2542   |  0.9197   |  0.8972   |  9.537    |  48.79    |  0.03386  |  45.23    |\n",
      "|  10       | -0.2549   |  0.9419   |  0.8676   |  9.957    |  49.37    |  0.08822  |  59.89    |\n",
      "|  11       | -0.2544   |  0.991    |  0.8991   |  9.442    |  37.27    |  0.004903 |  50.71    |\n",
      "|  12       | -0.2544   |  0.8723   |  0.8961   |  12.97    |  34.3     |  0.09425  |  59.8     |\n",
      "|  13       | -0.254    |  0.8194   |  0.8838   |  11.57    |  41.92    |  0.09113  |  45.27    |\n",
      "|  14       | -0.2521   |  0.8121   |  0.8805   |  12.27    |  30.2     |  0.03198  |  49.76    |\n",
      "|  15       | -0.2535   |  0.8328   |  0.8954   |  9.387    |  30.23    |  0.07655  |  46.9     |\n",
      "|  16       | -0.2537   |  0.807    |  0.8983   |  9.043    |  38.87    |  0.01686  |  52.19    |\n",
      "|  17       | -0.2534   |  0.8209   |  0.8667   |  12.46    |  30.28    |  0.07451  |  45.65    |\n",
      "|  18       | -0.2529   |  0.8277   |  0.8984   |  12.79    |  30.11    |  0.0377   |  45.47    |\n",
      "|  19       | -0.2539   |  0.8022   |  0.8948   |  9.528    |  30.13    |  0.04263  |  48.58    |\n",
      "|  20       | -0.2542   |  0.8715   |  0.8909   |  9.008    |  36.34    |  0.07021  |  45.27    |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# define Bayesian Optimization function and maximize the output of objective function\n",
    "# here, the sum of init_points and n_iter will equal the total number of rounds\n",
    "optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=7)\n",
    "optimizer.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8120618558202543,\n",
       " 'feature_fraction': 0.8804924233810146,\n",
       " 'max_depth': 12.267142340890745,\n",
       " 'min_child_weight': 30.200387989636972,\n",
       " 'min_split_gain': 0.031982756484661944,\n",
       " 'num_leaves': 49.75859581031999}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get optimized parameters\n",
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3500 rounds\n",
      "[10]\tvalid_0's l1: 0.338022\n",
      "[20]\tvalid_0's l1: 0.344689\n",
      "[30]\tvalid_0's l1: 0.344838\n",
      "[40]\tvalid_0's l1: 0.342264\n",
      "[50]\tvalid_0's l1: 0.34023\n",
      "[60]\tvalid_0's l1: 0.336653\n",
      "[70]\tvalid_0's l1: 0.333756\n",
      "[80]\tvalid_0's l1: 0.331738\n",
      "[90]\tvalid_0's l1: 0.329531\n",
      "[100]\tvalid_0's l1: 0.326918\n",
      "[110]\tvalid_0's l1: 0.324378\n",
      "[120]\tvalid_0's l1: 0.321728\n",
      "[130]\tvalid_0's l1: 0.31907\n",
      "[140]\tvalid_0's l1: 0.317121\n",
      "[150]\tvalid_0's l1: 0.315655\n",
      "[160]\tvalid_0's l1: 0.314135\n",
      "[170]\tvalid_0's l1: 0.312524\n",
      "[180]\tvalid_0's l1: 0.310824\n",
      "[190]\tvalid_0's l1: 0.309525\n",
      "[200]\tvalid_0's l1: 0.308348\n",
      "[210]\tvalid_0's l1: 0.307378\n",
      "[220]\tvalid_0's l1: 0.306174\n",
      "[230]\tvalid_0's l1: 0.305242\n",
      "[240]\tvalid_0's l1: 0.304308\n",
      "[250]\tvalid_0's l1: 0.300741\n",
      "[260]\tvalid_0's l1: 0.297999\n",
      "[270]\tvalid_0's l1: 0.298542\n",
      "[280]\tvalid_0's l1: 0.298478\n",
      "[290]\tvalid_0's l1: 0.298076\n",
      "[300]\tvalid_0's l1: 0.296737\n",
      "[310]\tvalid_0's l1: 0.295344\n",
      "[320]\tvalid_0's l1: 0.294641\n",
      "[330]\tvalid_0's l1: 0.293418\n",
      "[340]\tvalid_0's l1: 0.292227\n",
      "[350]\tvalid_0's l1: 0.291095\n",
      "[360]\tvalid_0's l1: 0.290178\n",
      "[370]\tvalid_0's l1: 0.289856\n",
      "[380]\tvalid_0's l1: 0.289545\n",
      "[390]\tvalid_0's l1: 0.289094\n",
      "[400]\tvalid_0's l1: 0.288439\n",
      "[410]\tvalid_0's l1: 0.287648\n",
      "[420]\tvalid_0's l1: 0.286894\n",
      "[430]\tvalid_0's l1: 0.285773\n",
      "[440]\tvalid_0's l1: 0.284936\n",
      "[450]\tvalid_0's l1: 0.284485\n",
      "[460]\tvalid_0's l1: 0.284157\n",
      "[470]\tvalid_0's l1: 0.283751\n",
      "[480]\tvalid_0's l1: 0.28353\n",
      "[490]\tvalid_0's l1: 0.281736\n",
      "[500]\tvalid_0's l1: 0.280256\n",
      "[510]\tvalid_0's l1: 0.280273\n",
      "[520]\tvalid_0's l1: 0.280042\n",
      "[530]\tvalid_0's l1: 0.279346\n",
      "[540]\tvalid_0's l1: 0.278379\n",
      "[550]\tvalid_0's l1: 0.277422\n",
      "[560]\tvalid_0's l1: 0.276761\n",
      "[570]\tvalid_0's l1: 0.277042\n",
      "[580]\tvalid_0's l1: 0.277169\n",
      "[590]\tvalid_0's l1: 0.27696\n",
      "[600]\tvalid_0's l1: 0.276749\n",
      "[610]\tvalid_0's l1: 0.276758\n",
      "[620]\tvalid_0's l1: 0.276775\n",
      "[630]\tvalid_0's l1: 0.276215\n",
      "[640]\tvalid_0's l1: 0.275694\n",
      "[650]\tvalid_0's l1: 0.274898\n",
      "[660]\tvalid_0's l1: 0.274242\n",
      "[670]\tvalid_0's l1: 0.274977\n",
      "[680]\tvalid_0's l1: 0.275404\n",
      "[690]\tvalid_0's l1: 0.275384\n",
      "[700]\tvalid_0's l1: 0.275133\n",
      "[710]\tvalid_0's l1: 0.27397\n",
      "[720]\tvalid_0's l1: 0.273179\n",
      "[730]\tvalid_0's l1: 0.273209\n",
      "[740]\tvalid_0's l1: 0.272768\n",
      "[750]\tvalid_0's l1: 0.27218\n",
      "[760]\tvalid_0's l1: 0.271604\n",
      "[770]\tvalid_0's l1: 0.271311\n",
      "[780]\tvalid_0's l1: 0.270994\n",
      "[790]\tvalid_0's l1: 0.270508\n",
      "[800]\tvalid_0's l1: 0.270156\n",
      "[810]\tvalid_0's l1: 0.270269\n",
      "[820]\tvalid_0's l1: 0.26998\n",
      "[830]\tvalid_0's l1: 0.269821\n",
      "[840]\tvalid_0's l1: 0.269677\n",
      "[850]\tvalid_0's l1: 0.269586\n",
      "[860]\tvalid_0's l1: 0.269297\n",
      "[870]\tvalid_0's l1: 0.270063\n",
      "[880]\tvalid_0's l1: 0.269998\n",
      "[890]\tvalid_0's l1: 0.268706\n",
      "[900]\tvalid_0's l1: 0.267504\n",
      "[910]\tvalid_0's l1: 0.268379\n",
      "[920]\tvalid_0's l1: 0.268795\n",
      "[930]\tvalid_0's l1: 0.268263\n",
      "[940]\tvalid_0's l1: 0.267537\n",
      "[950]\tvalid_0's l1: 0.267488\n",
      "[960]\tvalid_0's l1: 0.267278\n",
      "[970]\tvalid_0's l1: 0.26674\n",
      "[980]\tvalid_0's l1: 0.26636\n",
      "[990]\tvalid_0's l1: 0.266085\n",
      "[1000]\tvalid_0's l1: 0.265638\n",
      "[1010]\tvalid_0's l1: 0.264969\n",
      "[1020]\tvalid_0's l1: 0.264563\n",
      "[1030]\tvalid_0's l1: 0.264767\n",
      "[1040]\tvalid_0's l1: 0.264933\n",
      "[1050]\tvalid_0's l1: 0.264683\n",
      "[1060]\tvalid_0's l1: 0.264292\n",
      "[1070]\tvalid_0's l1: 0.264105\n",
      "[1080]\tvalid_0's l1: 0.263954\n",
      "[1090]\tvalid_0's l1: 0.263996\n",
      "[1100]\tvalid_0's l1: 0.263999\n",
      "[1110]\tvalid_0's l1: 0.26372\n",
      "[1120]\tvalid_0's l1: 0.26343\n",
      "[1130]\tvalid_0's l1: 0.262777\n",
      "[1140]\tvalid_0's l1: 0.262378\n",
      "[1150]\tvalid_0's l1: 0.262519\n",
      "[1160]\tvalid_0's l1: 0.262518\n",
      "[1170]\tvalid_0's l1: 0.261793\n",
      "[1180]\tvalid_0's l1: 0.261184\n",
      "[1190]\tvalid_0's l1: 0.261486\n",
      "[1200]\tvalid_0's l1: 0.261637\n",
      "[1210]\tvalid_0's l1: 0.26118\n",
      "[1220]\tvalid_0's l1: 0.260836\n",
      "[1230]\tvalid_0's l1: 0.260611\n",
      "[1240]\tvalid_0's l1: 0.260414\n",
      "[1250]\tvalid_0's l1: 0.260601\n",
      "[1260]\tvalid_0's l1: 0.260513\n",
      "[1270]\tvalid_0's l1: 0.260679\n",
      "[1280]\tvalid_0's l1: 0.260654\n",
      "[1290]\tvalid_0's l1: 0.260907\n",
      "[1300]\tvalid_0's l1: 0.260788\n",
      "[1310]\tvalid_0's l1: 0.261026\n",
      "[1320]\tvalid_0's l1: 0.261023\n",
      "[1330]\tvalid_0's l1: 0.260693\n",
      "[1340]\tvalid_0's l1: 0.260365\n",
      "[1350]\tvalid_0's l1: 0.260495\n",
      "[1360]\tvalid_0's l1: 0.260432\n",
      "[1370]\tvalid_0's l1: 0.260555\n",
      "[1380]\tvalid_0's l1: 0.260571\n",
      "[1390]\tvalid_0's l1: 0.260311\n",
      "[1400]\tvalid_0's l1: 0.260134\n",
      "[1410]\tvalid_0's l1: 0.259933\n",
      "[1420]\tvalid_0's l1: 0.259789\n",
      "[1430]\tvalid_0's l1: 0.259127\n",
      "[1440]\tvalid_0's l1: 0.258562\n",
      "[1450]\tvalid_0's l1: 0.258617\n",
      "[1460]\tvalid_0's l1: 0.258545\n",
      "[1470]\tvalid_0's l1: 0.258198\n",
      "[1480]\tvalid_0's l1: 0.257935\n",
      "[1490]\tvalid_0's l1: 0.257404\n",
      "[1500]\tvalid_0's l1: 0.257095\n",
      "[1510]\tvalid_0's l1: 0.257074\n",
      "[1520]\tvalid_0's l1: 0.257094\n",
      "[1530]\tvalid_0's l1: 0.256786\n",
      "[1540]\tvalid_0's l1: 0.256497\n",
      "[1550]\tvalid_0's l1: 0.256551\n",
      "[1560]\tvalid_0's l1: 0.256408\n",
      "[1570]\tvalid_0's l1: 0.255978\n",
      "[1580]\tvalid_0's l1: 0.25567\n",
      "[1590]\tvalid_0's l1: 0.256017\n",
      "[1600]\tvalid_0's l1: 0.256085\n",
      "[1610]\tvalid_0's l1: 0.255847\n",
      "[1620]\tvalid_0's l1: 0.255736\n",
      "[1630]\tvalid_0's l1: 0.25537\n",
      "[1640]\tvalid_0's l1: 0.254993\n",
      "[1650]\tvalid_0's l1: 0.255225\n",
      "[1660]\tvalid_0's l1: 0.255324\n",
      "[1670]\tvalid_0's l1: 0.255207\n",
      "[1680]\tvalid_0's l1: 0.255061\n",
      "[1690]\tvalid_0's l1: 0.255209\n",
      "[1700]\tvalid_0's l1: 0.255135\n",
      "[1710]\tvalid_0's l1: 0.255026\n",
      "[1720]\tvalid_0's l1: 0.254777\n",
      "[1730]\tvalid_0's l1: 0.25458\n",
      "[1740]\tvalid_0's l1: 0.254439\n",
      "[1750]\tvalid_0's l1: 0.254522\n",
      "[1760]\tvalid_0's l1: 0.254548\n",
      "[1770]\tvalid_0's l1: 0.254144\n",
      "[1780]\tvalid_0's l1: 0.253881\n",
      "[1790]\tvalid_0's l1: 0.254029\n",
      "[1800]\tvalid_0's l1: 0.253972\n",
      "[1810]\tvalid_0's l1: 0.254367\n",
      "[1820]\tvalid_0's l1: 0.254549\n",
      "[1830]\tvalid_0's l1: 0.25434\n",
      "[1840]\tvalid_0's l1: 0.254101\n",
      "[1850]\tvalid_0's l1: 0.25452\n",
      "[1860]\tvalid_0's l1: 0.254734\n",
      "[1870]\tvalid_0's l1: 0.254578\n",
      "[1880]\tvalid_0's l1: 0.254578\n",
      "[1890]\tvalid_0's l1: 0.254302\n",
      "[1900]\tvalid_0's l1: 0.254119\n",
      "[1910]\tvalid_0's l1: 0.253785\n",
      "[1920]\tvalid_0's l1: 0.253604\n",
      "[1930]\tvalid_0's l1: 0.253316\n",
      "[1940]\tvalid_0's l1: 0.253257\n",
      "[1950]\tvalid_0's l1: 0.253261\n",
      "[1960]\tvalid_0's l1: 0.253119\n",
      "[1970]\tvalid_0's l1: 0.25341\n",
      "[1980]\tvalid_0's l1: 0.253477\n",
      "[1990]\tvalid_0's l1: 0.253169\n",
      "[2000]\tvalid_0's l1: 0.252985\n",
      "[2010]\tvalid_0's l1: 0.253044\n",
      "[2020]\tvalid_0's l1: 0.252994\n",
      "[2030]\tvalid_0's l1: 0.252577\n",
      "[2040]\tvalid_0's l1: 0.252234\n",
      "[2050]\tvalid_0's l1: 0.252795\n",
      "[2060]\tvalid_0's l1: 0.25315\n",
      "[2070]\tvalid_0's l1: 0.252384\n",
      "[2080]\tvalid_0's l1: 0.251885\n",
      "[2090]\tvalid_0's l1: 0.252039\n",
      "[2100]\tvalid_0's l1: 0.251945\n",
      "[2110]\tvalid_0's l1: 0.252142\n",
      "[2120]\tvalid_0's l1: 0.252228\n",
      "[2130]\tvalid_0's l1: 0.25246\n",
      "[2140]\tvalid_0's l1: 0.252604\n",
      "[2150]\tvalid_0's l1: 0.252241\n",
      "[2160]\tvalid_0's l1: 0.252044\n",
      "[2170]\tvalid_0's l1: 0.252023\n",
      "[2180]\tvalid_0's l1: 0.251966\n",
      "[2190]\tvalid_0's l1: 0.251917\n",
      "[2200]\tvalid_0's l1: 0.251859\n",
      "[2210]\tvalid_0's l1: 0.251941\n",
      "[2220]\tvalid_0's l1: 0.251914\n",
      "[2230]\tvalid_0's l1: 0.252148\n",
      "[2240]\tvalid_0's l1: 0.252347\n",
      "[2250]\tvalid_0's l1: 0.251632\n",
      "[2260]\tvalid_0's l1: 0.251148\n",
      "[2270]\tvalid_0's l1: 0.251334\n",
      "[2280]\tvalid_0's l1: 0.251459\n",
      "[2290]\tvalid_0's l1: 0.251139\n",
      "[2300]\tvalid_0's l1: 0.250859\n",
      "[2310]\tvalid_0's l1: 0.251166\n",
      "[2320]\tvalid_0's l1: 0.251212\n",
      "[2330]\tvalid_0's l1: 0.25059\n",
      "[2340]\tvalid_0's l1: 0.250394\n",
      "[2350]\tvalid_0's l1: 0.250518\n",
      "[2360]\tvalid_0's l1: 0.250642\n",
      "[2370]\tvalid_0's l1: 0.250174\n",
      "[2380]\tvalid_0's l1: 0.249883\n",
      "[2390]\tvalid_0's l1: 0.249868\n",
      "[2400]\tvalid_0's l1: 0.249781\n",
      "[2410]\tvalid_0's l1: 0.249697\n",
      "[2420]\tvalid_0's l1: 0.249561\n",
      "[2430]\tvalid_0's l1: 0.249669\n",
      "[2440]\tvalid_0's l1: 0.249715\n",
      "[2450]\tvalid_0's l1: 0.249549\n",
      "[2460]\tvalid_0's l1: 0.249294\n",
      "[2470]\tvalid_0's l1: 0.249687\n",
      "[2480]\tvalid_0's l1: 0.249957\n",
      "[2490]\tvalid_0's l1: 0.249797\n",
      "[2500]\tvalid_0's l1: 0.249591\n",
      "[2510]\tvalid_0's l1: 0.249359\n",
      "[2520]\tvalid_0's l1: 0.24908\n",
      "[2530]\tvalid_0's l1: 0.24937\n",
      "[2540]\tvalid_0's l1: 0.249391\n",
      "[2550]\tvalid_0's l1: 0.249288\n",
      "[2560]\tvalid_0's l1: 0.249106\n",
      "[2570]\tvalid_0's l1: 0.248905\n",
      "[2580]\tvalid_0's l1: 0.248703\n",
      "[2590]\tvalid_0's l1: 0.249028\n",
      "[2600]\tvalid_0's l1: 0.249041\n",
      "[2610]\tvalid_0's l1: 0.248698\n",
      "[2620]\tvalid_0's l1: 0.248514\n",
      "[2630]\tvalid_0's l1: 0.24872\n",
      "[2640]\tvalid_0's l1: 0.248877\n",
      "[2650]\tvalid_0's l1: 0.248433\n",
      "[2660]\tvalid_0's l1: 0.248102\n",
      "[2670]\tvalid_0's l1: 0.248335\n",
      "[2680]\tvalid_0's l1: 0.248424\n",
      "[2690]\tvalid_0's l1: 0.248769\n",
      "[2700]\tvalid_0's l1: 0.248895\n",
      "[2710]\tvalid_0's l1: 0.248814\n",
      "[2720]\tvalid_0's l1: 0.248665\n",
      "[2730]\tvalid_0's l1: 0.248725\n",
      "[2740]\tvalid_0's l1: 0.248628\n",
      "[2750]\tvalid_0's l1: 0.248251\n",
      "[2760]\tvalid_0's l1: 0.248072\n",
      "[2770]\tvalid_0's l1: 0.24826\n",
      "[2780]\tvalid_0's l1: 0.248441\n",
      "[2790]\tvalid_0's l1: 0.248718\n",
      "[2800]\tvalid_0's l1: 0.248875\n",
      "[2810]\tvalid_0's l1: 0.248572\n",
      "[2820]\tvalid_0's l1: 0.24848\n",
      "[2830]\tvalid_0's l1: 0.248371\n",
      "[2840]\tvalid_0's l1: 0.248263\n",
      "[2850]\tvalid_0's l1: 0.248405\n",
      "[2860]\tvalid_0's l1: 0.248493\n",
      "[2870]\tvalid_0's l1: 0.248526\n",
      "[2880]\tvalid_0's l1: 0.24859\n",
      "[2890]\tvalid_0's l1: 0.24832\n",
      "[2900]\tvalid_0's l1: 0.248256\n",
      "[2910]\tvalid_0's l1: 0.248053\n",
      "[2920]\tvalid_0's l1: 0.247843\n",
      "[2930]\tvalid_0's l1: 0.247857\n",
      "[2940]\tvalid_0's l1: 0.247955\n",
      "[2950]\tvalid_0's l1: 0.247793\n",
      "[2960]\tvalid_0's l1: 0.247558\n",
      "[2970]\tvalid_0's l1: 0.247648\n",
      "[2980]\tvalid_0's l1: 0.247696\n",
      "[2990]\tvalid_0's l1: 0.247768\n",
      "[3000]\tvalid_0's l1: 0.247838\n",
      "[3010]\tvalid_0's l1: 0.247396\n",
      "[3020]\tvalid_0's l1: 0.247103\n",
      "[3030]\tvalid_0's l1: 0.246783\n",
      "[3040]\tvalid_0's l1: 0.246584\n",
      "[3050]\tvalid_0's l1: 0.247091\n",
      "[3060]\tvalid_0's l1: 0.247452\n",
      "[3070]\tvalid_0's l1: 0.247472\n",
      "[3080]\tvalid_0's l1: 0.247462\n",
      "[3090]\tvalid_0's l1: 0.247417\n",
      "[3100]\tvalid_0's l1: 0.247397\n",
      "[3110]\tvalid_0's l1: 0.247298\n",
      "[3120]\tvalid_0's l1: 0.247298\n",
      "[3130]\tvalid_0's l1: 0.247616\n",
      "[3140]\tvalid_0's l1: 0.247727\n",
      "[3150]\tvalid_0's l1: 0.247184\n",
      "[3160]\tvalid_0's l1: 0.246908\n",
      "[3170]\tvalid_0's l1: 0.247022\n",
      "[3180]\tvalid_0's l1: 0.24697\n",
      "[3190]\tvalid_0's l1: 0.247009\n",
      "[3200]\tvalid_0's l1: 0.247008\n",
      "[3210]\tvalid_0's l1: 0.247113\n",
      "[3220]\tvalid_0's l1: 0.247104\n",
      "[3230]\tvalid_0's l1: 0.246733\n",
      "[3240]\tvalid_0's l1: 0.246467\n",
      "[3250]\tvalid_0's l1: 0.246503\n",
      "[3260]\tvalid_0's l1: 0.246546\n",
      "[3270]\tvalid_0's l1: 0.246325\n",
      "[3280]\tvalid_0's l1: 0.246177\n",
      "[3290]\tvalid_0's l1: 0.246346\n",
      "[3300]\tvalid_0's l1: 0.246453\n",
      "[3310]\tvalid_0's l1: 0.246277\n",
      "[3320]\tvalid_0's l1: 0.246062\n",
      "[3330]\tvalid_0's l1: 0.245807\n",
      "[3340]\tvalid_0's l1: 0.245727\n",
      "[3350]\tvalid_0's l1: 0.245835\n",
      "[3360]\tvalid_0's l1: 0.245936\n",
      "[3370]\tvalid_0's l1: 0.246154\n",
      "[3380]\tvalid_0's l1: 0.246225\n",
      "[3390]\tvalid_0's l1: 0.246191\n",
      "[3400]\tvalid_0's l1: 0.246109\n",
      "[3410]\tvalid_0's l1: 0.24618\n",
      "[3420]\tvalid_0's l1: 0.246103\n",
      "[3430]\tvalid_0's l1: 0.245985\n",
      "[3440]\tvalid_0's l1: 0.245842\n",
      "[3450]\tvalid_0's l1: 0.245444\n",
      "[3460]\tvalid_0's l1: 0.245159\n",
      "[3470]\tvalid_0's l1: 0.245779\n",
      "[3480]\tvalid_0's l1: 0.246173\n",
      "[3490]\tvalid_0's l1: 0.246098\n",
      "[3500]\tvalid_0's l1: 0.245996\n",
      "[3510]\tvalid_0's l1: 0.245593\n",
      "[3520]\tvalid_0's l1: 0.245457\n",
      "[3530]\tvalid_0's l1: 0.245761\n",
      "[3540]\tvalid_0's l1: 0.245892\n",
      "[3550]\tvalid_0's l1: 0.246023\n",
      "[3560]\tvalid_0's l1: 0.246121\n",
      "[3570]\tvalid_0's l1: 0.245856\n",
      "[3580]\tvalid_0's l1: 0.24568\n",
      "[3590]\tvalid_0's l1: 0.245738\n",
      "[3600]\tvalid_0's l1: 0.245732\n",
      "[3610]\tvalid_0's l1: 0.245569\n",
      "[3620]\tvalid_0's l1: 0.245504\n",
      "[3630]\tvalid_0's l1: 0.244726\n",
      "[3640]\tvalid_0's l1: 0.244339\n",
      "[3650]\tvalid_0's l1: 0.244827\n",
      "[3660]\tvalid_0's l1: 0.245172\n",
      "[3670]\tvalid_0's l1: 0.245017\n",
      "[3680]\tvalid_0's l1: 0.244963\n",
      "[3690]\tvalid_0's l1: 0.244773\n",
      "[3700]\tvalid_0's l1: 0.244586\n",
      "[3710]\tvalid_0's l1: 0.245147\n",
      "[3720]\tvalid_0's l1: 0.245537\n",
      "[3730]\tvalid_0's l1: 0.245531\n",
      "[3740]\tvalid_0's l1: 0.24554\n",
      "[3750]\tvalid_0's l1: 0.245092\n",
      "[3760]\tvalid_0's l1: 0.244798\n",
      "[3770]\tvalid_0's l1: 0.245147\n",
      "[3780]\tvalid_0's l1: 0.245349\n",
      "[3790]\tvalid_0's l1: 0.244661\n",
      "[3800]\tvalid_0's l1: 0.244269\n",
      "[3810]\tvalid_0's l1: 0.244724\n",
      "[3820]\tvalid_0's l1: 0.245047\n",
      "[3830]\tvalid_0's l1: 0.245053\n",
      "[3840]\tvalid_0's l1: 0.245013\n",
      "[3850]\tvalid_0's l1: 0.245043\n",
      "[3860]\tvalid_0's l1: 0.245096\n",
      "[3870]\tvalid_0's l1: 0.245149\n",
      "[3880]\tvalid_0's l1: 0.245184\n",
      "[3890]\tvalid_0's l1: 0.245245\n",
      "[3900]\tvalid_0's l1: 0.245221\n",
      "[3910]\tvalid_0's l1: 0.245153\n",
      "[3920]\tvalid_0's l1: 0.245048\n",
      "[3930]\tvalid_0's l1: 0.245179\n",
      "[3940]\tvalid_0's l1: 0.245248\n",
      "[3950]\tvalid_0's l1: 0.244951\n",
      "[3960]\tvalid_0's l1: 0.244707\n",
      "[3970]\tvalid_0's l1: 0.244616\n",
      "[3980]\tvalid_0's l1: 0.244483\n",
      "[3990]\tvalid_0's l1: 0.244212\n",
      "[4000]\tvalid_0's l1: 0.243972\n",
      "[4010]\tvalid_0's l1: 0.244236\n",
      "[4020]\tvalid_0's l1: 0.244371\n",
      "[4030]\tvalid_0's l1: 0.244506\n",
      "[4040]\tvalid_0's l1: 0.244669\n",
      "[4050]\tvalid_0's l1: 0.244737\n",
      "[4060]\tvalid_0's l1: 0.244798\n",
      "[4070]\tvalid_0's l1: 0.244679\n",
      "[4080]\tvalid_0's l1: 0.244643\n",
      "[4090]\tvalid_0's l1: 0.244539\n",
      "[4100]\tvalid_0's l1: 0.244425\n",
      "[4110]\tvalid_0's l1: 0.244371\n",
      "[4120]\tvalid_0's l1: 0.244364\n",
      "[4130]\tvalid_0's l1: 0.244365\n",
      "[4140]\tvalid_0's l1: 0.2443\n",
      "[4150]\tvalid_0's l1: 0.244221\n",
      "[4160]\tvalid_0's l1: 0.244135\n",
      "[4170]\tvalid_0's l1: 0.244053\n",
      "[4180]\tvalid_0's l1: 0.243934\n",
      "[4190]\tvalid_0's l1: 0.243979\n",
      "[4200]\tvalid_0's l1: 0.244023\n",
      "[4210]\tvalid_0's l1: 0.244697\n",
      "[4220]\tvalid_0's l1: 0.245017\n",
      "[4230]\tvalid_0's l1: 0.245007\n",
      "[4240]\tvalid_0's l1: 0.245028\n",
      "[4250]\tvalid_0's l1: 0.244757\n",
      "[4260]\tvalid_0's l1: 0.244554\n",
      "[4270]\tvalid_0's l1: 0.244476\n",
      "[4280]\tvalid_0's l1: 0.244399\n",
      "[4290]\tvalid_0's l1: 0.244176\n",
      "[4300]\tvalid_0's l1: 0.244024\n",
      "[4310]\tvalid_0's l1: 0.243974\n",
      "[4320]\tvalid_0's l1: 0.243943\n",
      "[4330]\tvalid_0's l1: 0.244101\n",
      "[4340]\tvalid_0's l1: 0.244271\n",
      "[4350]\tvalid_0's l1: 0.244376\n",
      "[4360]\tvalid_0's l1: 0.244382\n",
      "[4370]\tvalid_0's l1: 0.24432\n",
      "[4380]\tvalid_0's l1: 0.244247\n",
      "[4390]\tvalid_0's l1: 0.244207\n",
      "[4400]\tvalid_0's l1: 0.244188\n",
      "[4410]\tvalid_0's l1: 0.244161\n",
      "[4420]\tvalid_0's l1: 0.244155\n",
      "[4430]\tvalid_0's l1: 0.243894\n",
      "[4440]\tvalid_0's l1: 0.243671\n",
      "[4450]\tvalid_0's l1: 0.243976\n",
      "[4460]\tvalid_0's l1: 0.244132\n",
      "[4470]\tvalid_0's l1: 0.243871\n",
      "[4480]\tvalid_0's l1: 0.243764\n",
      "[4490]\tvalid_0's l1: 0.243778\n",
      "[4500]\tvalid_0's l1: 0.243743\n",
      "[4510]\tvalid_0's l1: 0.243863\n",
      "[4520]\tvalid_0's l1: 0.244039\n",
      "[4530]\tvalid_0's l1: 0.244219\n",
      "[4540]\tvalid_0's l1: 0.244264\n",
      "[4550]\tvalid_0's l1: 0.243968\n",
      "[4560]\tvalid_0's l1: 0.243758\n",
      "[4570]\tvalid_0's l1: 0.243887\n",
      "[4580]\tvalid_0's l1: 0.243967\n",
      "[4590]\tvalid_0's l1: 0.244184\n",
      "[4600]\tvalid_0's l1: 0.244256\n",
      "[4610]\tvalid_0's l1: 0.243836\n",
      "[4620]\tvalid_0's l1: 0.243653\n",
      "[4630]\tvalid_0's l1: 0.243786\n",
      "[4640]\tvalid_0's l1: 0.243948\n",
      "[4650]\tvalid_0's l1: 0.243808\n",
      "[4660]\tvalid_0's l1: 0.243708\n",
      "[4670]\tvalid_0's l1: 0.243577\n",
      "[4680]\tvalid_0's l1: 0.243418\n",
      "[4690]\tvalid_0's l1: 0.243652\n",
      "[4700]\tvalid_0's l1: 0.243774\n",
      "[4710]\tvalid_0's l1: 0.24334\n",
      "[4720]\tvalid_0's l1: 0.243063\n",
      "[4730]\tvalid_0's l1: 0.243268\n",
      "[4740]\tvalid_0's l1: 0.243384\n",
      "[4750]\tvalid_0's l1: 0.243083\n",
      "[4760]\tvalid_0's l1: 0.242828\n",
      "[4770]\tvalid_0's l1: 0.243077\n",
      "[4780]\tvalid_0's l1: 0.243259\n",
      "[4790]\tvalid_0's l1: 0.243072\n",
      "[4800]\tvalid_0's l1: 0.242898\n",
      "[4810]\tvalid_0's l1: 0.24287\n",
      "[4820]\tvalid_0's l1: 0.242837\n",
      "[4830]\tvalid_0's l1: 0.242953\n",
      "[4840]\tvalid_0's l1: 0.242999\n",
      "[4850]\tvalid_0's l1: 0.243052\n",
      "[4860]\tvalid_0's l1: 0.24303\n",
      "[4870]\tvalid_0's l1: 0.243022\n",
      "[4880]\tvalid_0's l1: 0.243048\n",
      "[4890]\tvalid_0's l1: 0.243048\n",
      "[4900]\tvalid_0's l1: 0.243155\n",
      "[4910]\tvalid_0's l1: 0.242901\n",
      "[4920]\tvalid_0's l1: 0.24269\n",
      "[4930]\tvalid_0's l1: 0.242952\n",
      "[4940]\tvalid_0's l1: 0.243186\n",
      "[4950]\tvalid_0's l1: 0.243431\n",
      "[4960]\tvalid_0's l1: 0.243583\n",
      "[4970]\tvalid_0's l1: 0.24273\n",
      "[4980]\tvalid_0's l1: 0.242258\n",
      "[4990]\tvalid_0's l1: 0.242438\n",
      "[5000]\tvalid_0's l1: 0.242445\n",
      "[5010]\tvalid_0's l1: 0.242574\n",
      "[5020]\tvalid_0's l1: 0.242576\n",
      "[5030]\tvalid_0's l1: 0.242956\n",
      "[5040]\tvalid_0's l1: 0.2431\n",
      "[5050]\tvalid_0's l1: 0.243171\n",
      "[5060]\tvalid_0's l1: 0.243194\n",
      "[5070]\tvalid_0's l1: 0.242961\n",
      "[5080]\tvalid_0's l1: 0.242827\n",
      "[5090]\tvalid_0's l1: 0.243069\n",
      "[5100]\tvalid_0's l1: 0.243154\n",
      "[5110]\tvalid_0's l1: 0.243021\n",
      "[5120]\tvalid_0's l1: 0.242993\n",
      "[5130]\tvalid_0's l1: 0.242555\n",
      "[5140]\tvalid_0's l1: 0.242379\n",
      "[5150]\tvalid_0's l1: 0.241984\n",
      "[5160]\tvalid_0's l1: 0.241762\n",
      "[5170]\tvalid_0's l1: 0.241985\n",
      "[5180]\tvalid_0's l1: 0.24202\n",
      "[5190]\tvalid_0's l1: 0.242287\n",
      "[5200]\tvalid_0's l1: 0.24243\n",
      "[5210]\tvalid_0's l1: 0.242224\n",
      "[5220]\tvalid_0's l1: 0.242118\n",
      "[5230]\tvalid_0's l1: 0.242399\n",
      "[5240]\tvalid_0's l1: 0.242676\n",
      "[5250]\tvalid_0's l1: 0.242672\n",
      "[5260]\tvalid_0's l1: 0.24259\n",
      "[5270]\tvalid_0's l1: 0.242526\n",
      "[5280]\tvalid_0's l1: 0.242479\n",
      "[5290]\tvalid_0's l1: 0.242296\n",
      "[5300]\tvalid_0's l1: 0.242196\n",
      "[5310]\tvalid_0's l1: 0.242362\n",
      "[5320]\tvalid_0's l1: 0.242474\n",
      "[5330]\tvalid_0's l1: 0.242392\n",
      "[5340]\tvalid_0's l1: 0.242271\n",
      "[5350]\tvalid_0's l1: 0.242576\n",
      "[5360]\tvalid_0's l1: 0.242811\n",
      "[5370]\tvalid_0's l1: 0.242363\n",
      "[5380]\tvalid_0's l1: 0.242037\n",
      "[5390]\tvalid_0's l1: 0.242292\n",
      "[5400]\tvalid_0's l1: 0.242445\n",
      "[5410]\tvalid_0's l1: 0.242347\n",
      "[5420]\tvalid_0's l1: 0.242258\n",
      "[5430]\tvalid_0's l1: 0.242357\n",
      "[5440]\tvalid_0's l1: 0.242358\n",
      "[5450]\tvalid_0's l1: 0.242216\n",
      "[5460]\tvalid_0's l1: 0.24213\n",
      "[5470]\tvalid_0's l1: 0.241688\n",
      "[5480]\tvalid_0's l1: 0.241471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5490]\tvalid_0's l1: 0.241603\n",
      "[5500]\tvalid_0's l1: 0.241673\n",
      "[5510]\tvalid_0's l1: 0.24188\n",
      "[5520]\tvalid_0's l1: 0.242085\n",
      "[5530]\tvalid_0's l1: 0.242177\n",
      "[5540]\tvalid_0's l1: 0.24222\n",
      "[5550]\tvalid_0's l1: 0.241869\n",
      "[5560]\tvalid_0's l1: 0.241642\n",
      "[5570]\tvalid_0's l1: 0.24203\n",
      "[5580]\tvalid_0's l1: 0.242248\n",
      "[5590]\tvalid_0's l1: 0.242171\n",
      "[5600]\tvalid_0's l1: 0.242114\n",
      "[5610]\tvalid_0's l1: 0.24214\n",
      "[5620]\tvalid_0's l1: 0.2422\n",
      "[5630]\tvalid_0's l1: 0.242095\n",
      "[5640]\tvalid_0's l1: 0.242088\n",
      "[5650]\tvalid_0's l1: 0.241864\n",
      "[5660]\tvalid_0's l1: 0.241789\n",
      "[5670]\tvalid_0's l1: 0.241748\n",
      "[5680]\tvalid_0's l1: 0.241679\n",
      "[5690]\tvalid_0's l1: 0.24176\n",
      "[5700]\tvalid_0's l1: 0.241758\n",
      "[5710]\tvalid_0's l1: 0.241403\n",
      "[5720]\tvalid_0's l1: 0.241169\n",
      "[5730]\tvalid_0's l1: 0.241428\n",
      "[5740]\tvalid_0's l1: 0.241556\n",
      "[5750]\tvalid_0's l1: 0.241546\n",
      "[5760]\tvalid_0's l1: 0.24157\n",
      "[5770]\tvalid_0's l1: 0.241799\n",
      "[5780]\tvalid_0's l1: 0.2419\n",
      "[5790]\tvalid_0's l1: 0.241916\n",
      "[5800]\tvalid_0's l1: 0.241886\n",
      "[5810]\tvalid_0's l1: 0.241829\n",
      "[5820]\tvalid_0's l1: 0.241746\n",
      "[5830]\tvalid_0's l1: 0.241554\n",
      "[5840]\tvalid_0's l1: 0.241355\n",
      "[5850]\tvalid_0's l1: 0.241253\n",
      "[5860]\tvalid_0's l1: 0.241259\n",
      "[5870]\tvalid_0's l1: 0.241409\n",
      "[5880]\tvalid_0's l1: 0.241456\n",
      "[5890]\tvalid_0's l1: 0.241387\n",
      "[5900]\tvalid_0's l1: 0.241339\n",
      "[5910]\tvalid_0's l1: 0.241422\n",
      "[5920]\tvalid_0's l1: 0.241582\n",
      "[5930]\tvalid_0's l1: 0.241558\n",
      "[5940]\tvalid_0's l1: 0.241559\n",
      "[5950]\tvalid_0's l1: 0.241695\n",
      "[5960]\tvalid_0's l1: 0.241707\n",
      "[5970]\tvalid_0's l1: 0.241702\n",
      "[5980]\tvalid_0's l1: 0.241655\n",
      "[5990]\tvalid_0's l1: 0.241449\n",
      "[6000]\tvalid_0's l1: 0.241251\n",
      "[6010]\tvalid_0's l1: 0.241221\n",
      "[6020]\tvalid_0's l1: 0.241208\n",
      "[6030]\tvalid_0's l1: 0.241439\n",
      "[6040]\tvalid_0's l1: 0.24157\n",
      "[6050]\tvalid_0's l1: 0.241781\n",
      "[6060]\tvalid_0's l1: 0.241948\n",
      "[6070]\tvalid_0's l1: 0.241677\n",
      "[6080]\tvalid_0's l1: 0.241628\n",
      "[6090]\tvalid_0's l1: 0.241783\n",
      "[6100]\tvalid_0's l1: 0.241971\n",
      "[6110]\tvalid_0's l1: 0.242058\n",
      "[6120]\tvalid_0's l1: 0.242094\n",
      "[6130]\tvalid_0's l1: 0.242148\n",
      "[6140]\tvalid_0's l1: 0.242145\n",
      "[6150]\tvalid_0's l1: 0.241914\n",
      "[6160]\tvalid_0's l1: 0.241729\n",
      "[6170]\tvalid_0's l1: 0.241606\n",
      "[6180]\tvalid_0's l1: 0.24156\n",
      "[6190]\tvalid_0's l1: 0.241469\n",
      "[6200]\tvalid_0's l1: 0.24143\n",
      "[6210]\tvalid_0's l1: 0.24186\n",
      "[6220]\tvalid_0's l1: 0.242023\n",
      "[6230]\tvalid_0's l1: 0.242084\n",
      "[6240]\tvalid_0's l1: 0.24214\n",
      "[6250]\tvalid_0's l1: 0.242069\n",
      "[6260]\tvalid_0's l1: 0.242073\n",
      "[6270]\tvalid_0's l1: 0.242185\n",
      "[6280]\tvalid_0's l1: 0.242254\n",
      "[6290]\tvalid_0's l1: 0.242102\n",
      "[6300]\tvalid_0's l1: 0.241957\n",
      "[6310]\tvalid_0's l1: 0.242256\n",
      "[6320]\tvalid_0's l1: 0.242419\n",
      "[6330]\tvalid_0's l1: 0.242102\n",
      "[6340]\tvalid_0's l1: 0.241944\n",
      "[6350]\tvalid_0's l1: 0.24158\n",
      "[6360]\tvalid_0's l1: 0.241336\n",
      "[6370]\tvalid_0's l1: 0.241481\n",
      "[6380]\tvalid_0's l1: 0.241523\n",
      "[6390]\tvalid_0's l1: 0.241518\n",
      "[6400]\tvalid_0's l1: 0.241517\n",
      "[6410]\tvalid_0's l1: 0.241364\n",
      "[6420]\tvalid_0's l1: 0.241187\n",
      "[6430]\tvalid_0's l1: 0.241351\n",
      "[6440]\tvalid_0's l1: 0.241506\n",
      "[6450]\tvalid_0's l1: 0.24118\n",
      "[6460]\tvalid_0's l1: 0.240989\n",
      "[6470]\tvalid_0's l1: 0.241168\n",
      "[6480]\tvalid_0's l1: 0.241217\n",
      "[6490]\tvalid_0's l1: 0.241581\n",
      "[6500]\tvalid_0's l1: 0.241871\n",
      "[6510]\tvalid_0's l1: 0.241404\n",
      "[6520]\tvalid_0's l1: 0.241039\n",
      "[6530]\tvalid_0's l1: 0.240788\n",
      "[6540]\tvalid_0's l1: 0.240676\n",
      "[6550]\tvalid_0's l1: 0.240798\n",
      "[6560]\tvalid_0's l1: 0.24087\n",
      "[6570]\tvalid_0's l1: 0.240894\n",
      "[6580]\tvalid_0's l1: 0.24092\n",
      "[6590]\tvalid_0's l1: 0.241162\n",
      "[6600]\tvalid_0's l1: 0.241407\n",
      "[6610]\tvalid_0's l1: 0.241724\n",
      "[6620]\tvalid_0's l1: 0.241968\n",
      "[6630]\tvalid_0's l1: 0.242038\n",
      "[6640]\tvalid_0's l1: 0.242014\n",
      "[6650]\tvalid_0's l1: 0.241443\n",
      "[6660]\tvalid_0's l1: 0.241097\n",
      "[6670]\tvalid_0's l1: 0.24109\n",
      "[6680]\tvalid_0's l1: 0.240974\n",
      "[6690]\tvalid_0's l1: 0.241102\n",
      "[6700]\tvalid_0's l1: 0.241257\n",
      "[6710]\tvalid_0's l1: 0.241255\n",
      "[6720]\tvalid_0's l1: 0.241208\n",
      "[6730]\tvalid_0's l1: 0.241241\n",
      "[6740]\tvalid_0's l1: 0.241162\n",
      "[6750]\tvalid_0's l1: 0.240744\n",
      "[6760]\tvalid_0's l1: 0.240541\n",
      "[6770]\tvalid_0's l1: 0.240946\n",
      "[6780]\tvalid_0's l1: 0.24126\n",
      "[6790]\tvalid_0's l1: 0.241356\n",
      "[6800]\tvalid_0's l1: 0.241426\n",
      "[6810]\tvalid_0's l1: 0.24139\n",
      "[6820]\tvalid_0's l1: 0.241347\n",
      "[6830]\tvalid_0's l1: 0.241025\n",
      "[6840]\tvalid_0's l1: 0.240875\n",
      "[6850]\tvalid_0's l1: 0.240947\n",
      "[6860]\tvalid_0's l1: 0.241012\n",
      "[6870]\tvalid_0's l1: 0.240986\n",
      "[6880]\tvalid_0's l1: 0.240968\n",
      "[6890]\tvalid_0's l1: 0.240937\n",
      "[6900]\tvalid_0's l1: 0.240928\n",
      "[6910]\tvalid_0's l1: 0.241177\n",
      "[6920]\tvalid_0's l1: 0.241337\n",
      "[6930]\tvalid_0's l1: 0.241137\n",
      "[6940]\tvalid_0's l1: 0.241025\n",
      "[6950]\tvalid_0's l1: 0.241106\n",
      "[6960]\tvalid_0's l1: 0.241149\n",
      "[6970]\tvalid_0's l1: 0.241292\n",
      "[6980]\tvalid_0's l1: 0.241382\n",
      "[6990]\tvalid_0's l1: 0.241282\n",
      "[7000]\tvalid_0's l1: 0.24126\n",
      "[7010]\tvalid_0's l1: 0.241052\n",
      "[7020]\tvalid_0's l1: 0.240946\n",
      "[7030]\tvalid_0's l1: 0.241155\n",
      "[7040]\tvalid_0's l1: 0.241389\n",
      "[7050]\tvalid_0's l1: 0.241175\n",
      "[7060]\tvalid_0's l1: 0.241039\n",
      "[7070]\tvalid_0's l1: 0.240954\n",
      "[7080]\tvalid_0's l1: 0.240921\n",
      "[7090]\tvalid_0's l1: 0.240956\n",
      "[7100]\tvalid_0's l1: 0.240927\n",
      "[7110]\tvalid_0's l1: 0.241034\n",
      "[7120]\tvalid_0's l1: 0.241051\n",
      "[7130]\tvalid_0's l1: 0.240835\n",
      "[7140]\tvalid_0's l1: 0.240663\n",
      "[7150]\tvalid_0's l1: 0.240801\n",
      "[7160]\tvalid_0's l1: 0.240854\n",
      "[7170]\tvalid_0's l1: 0.240797\n",
      "[7180]\tvalid_0's l1: 0.240762\n",
      "[7190]\tvalid_0's l1: 0.240665\n",
      "[7200]\tvalid_0's l1: 0.24062\n",
      "[7210]\tvalid_0's l1: 0.240387\n",
      "[7220]\tvalid_0's l1: 0.240271\n",
      "[7230]\tvalid_0's l1: 0.240466\n",
      "[7240]\tvalid_0's l1: 0.240671\n",
      "[7250]\tvalid_0's l1: 0.240713\n",
      "[7260]\tvalid_0's l1: 0.240754\n",
      "[7270]\tvalid_0's l1: 0.240827\n",
      "[7280]\tvalid_0's l1: 0.240888\n",
      "[7290]\tvalid_0's l1: 0.240825\n",
      "[7300]\tvalid_0's l1: 0.240798\n",
      "[7310]\tvalid_0's l1: 0.24073\n",
      "[7320]\tvalid_0's l1: 0.240639\n",
      "[7330]\tvalid_0's l1: 0.24097\n",
      "[7340]\tvalid_0's l1: 0.241176\n",
      "[7350]\tvalid_0's l1: 0.241212\n",
      "[7360]\tvalid_0's l1: 0.24122\n",
      "[7370]\tvalid_0's l1: 0.24072\n",
      "[7380]\tvalid_0's l1: 0.240421\n",
      "[7390]\tvalid_0's l1: 0.240642\n",
      "[7400]\tvalid_0's l1: 0.240758\n",
      "[7410]\tvalid_0's l1: 0.241027\n",
      "[7420]\tvalid_0's l1: 0.241146\n",
      "[7430]\tvalid_0's l1: 0.241461\n",
      "[7440]\tvalid_0's l1: 0.241537\n",
      "[7450]\tvalid_0's l1: 0.241464\n",
      "[7460]\tvalid_0's l1: 0.241377\n",
      "[7470]\tvalid_0's l1: 0.24114\n",
      "[7480]\tvalid_0's l1: 0.240995\n",
      "[7490]\tvalid_0's l1: 0.240764\n",
      "[7500]\tvalid_0's l1: 0.240697\n",
      "[7510]\tvalid_0's l1: 0.240317\n",
      "[7520]\tvalid_0's l1: 0.240055\n",
      "[7530]\tvalid_0's l1: 0.240275\n",
      "[7540]\tvalid_0's l1: 0.240423\n",
      "[7550]\tvalid_0's l1: 0.240369\n",
      "[7560]\tvalid_0's l1: 0.240425\n",
      "[7570]\tvalid_0's l1: 0.240539\n",
      "[7580]\tvalid_0's l1: 0.240556\n",
      "[7590]\tvalid_0's l1: 0.240681\n",
      "[7600]\tvalid_0's l1: 0.240745\n",
      "[7610]\tvalid_0's l1: 0.241021\n",
      "[7620]\tvalid_0's l1: 0.241073\n",
      "[7630]\tvalid_0's l1: 0.240659\n",
      "[7640]\tvalid_0's l1: 0.240448\n",
      "[7650]\tvalid_0's l1: 0.24037\n",
      "[7660]\tvalid_0's l1: 0.240359\n",
      "[7670]\tvalid_0's l1: 0.24061\n",
      "[7680]\tvalid_0's l1: 0.240761\n",
      "[7690]\tvalid_0's l1: 0.240497\n",
      "[7700]\tvalid_0's l1: 0.240281\n",
      "[7710]\tvalid_0's l1: 0.240079\n",
      "[7720]\tvalid_0's l1: 0.240056\n",
      "[7730]\tvalid_0's l1: 0.240221\n",
      "[7740]\tvalid_0's l1: 0.240299\n",
      "[7750]\tvalid_0's l1: 0.240393\n",
      "[7760]\tvalid_0's l1: 0.24042\n",
      "[7770]\tvalid_0's l1: 0.240138\n",
      "[7780]\tvalid_0's l1: 0.239918\n",
      "[7790]\tvalid_0's l1: 0.23992\n",
      "[7800]\tvalid_0's l1: 0.239898\n",
      "[7810]\tvalid_0's l1: 0.239797\n",
      "[7820]\tvalid_0's l1: 0.239717\n",
      "[7830]\tvalid_0's l1: 0.239964\n",
      "[7840]\tvalid_0's l1: 0.240087\n",
      "[7850]\tvalid_0's l1: 0.240216\n",
      "[7860]\tvalid_0's l1: 0.240248\n",
      "[7870]\tvalid_0's l1: 0.240052\n",
      "[7880]\tvalid_0's l1: 0.239941\n",
      "[7890]\tvalid_0's l1: 0.240053\n",
      "[7900]\tvalid_0's l1: 0.240062\n",
      "[7910]\tvalid_0's l1: 0.239776\n",
      "[7920]\tvalid_0's l1: 0.239683\n",
      "[7930]\tvalid_0's l1: 0.240087\n",
      "[7940]\tvalid_0's l1: 0.240306\n",
      "[7950]\tvalid_0's l1: 0.240669\n",
      "[7960]\tvalid_0's l1: 0.240873\n",
      "[7970]\tvalid_0's l1: 0.240915\n",
      "[7980]\tvalid_0's l1: 0.240967\n",
      "[7990]\tvalid_0's l1: 0.240646\n",
      "[8000]\tvalid_0's l1: 0.240459\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7920]\tvalid_0's l1: 0.239683\n"
     ]
    }
   ],
   "source": [
    "# final parameters based on optimization\n",
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'l1',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 46,\n",
    "    'feature_fraction': 0.8804924233810146,\n",
    "    'bagging_fraction': 0.8120618558202543,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 1,\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 30.200387989636972,\n",
    "    'min_split_gain': 0.031982756484661944\n",
    "}\n",
    "\n",
    "# train model\n",
    "lgbm = lgb.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets=test_data,\n",
    "                       verbose_eval=10,\n",
    "                       num_boost_round=8000,\n",
    "                       early_stopping_rounds=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.41854934601664684\n",
      "Recall:  0.46437994722955145\n",
      "Prec.:  0.38095238095238093\n",
      "Acc.:  0.7883116883116883\n"
     ]
    }
   ],
   "source": [
    "# predict on X_test and classify based on a threshold of 0.6\n",
    "y_pred = lgbm.predict(X_test)\n",
    "y_pred[y_pred >= 0.6] = 1\n",
    "y_pred[y_pred < 0.6] = 0\n",
    "\n",
    "print (\"F1: \", f1_score(y_pred, y_test))\n",
    "print (\"Recall: \", recall_score(y_pred, y_test))\n",
    "print (\"Prec.: \", precision_score(y_pred, y_test))\n",
    "print (\"Acc.: \", accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction(adopter)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.005829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.141774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86676</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86677</td>\n",
       "      <td>0.843112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86678</td>\n",
       "      <td>0.036194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86679</td>\n",
       "      <td>0.319560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86680</td>\n",
       "      <td>0.897001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86681 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction(adopter)\n",
       "0                 0.005829\n",
       "1                 0.007001\n",
       "2                 0.141774\n",
       "3                 0.000410\n",
       "4                 0.000414\n",
       "...                    ...\n",
       "86676             0.000778\n",
       "86677             0.843112\n",
       "86678             0.036194\n",
       "86679             0.319560\n",
       "86680             0.897001\n",
       "\n",
       "[86681 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now import test set without labels and make predictions\n",
    "test = pd.read_csv('HW4 - test data.csv',header=0)\n",
    "\n",
    "y_pred = lgbm.predict(test)\n",
    "\n",
    "y_pred = pd.DataFrame({'prediction(adopter)': y_pred })\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final predictions for ensembling later in Excel\n",
    "np.savetxt(\"lightgbm.csv\", y_pred , delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this tutorial as a resource for tuning my parameters for Catboost using Hyperopt:\n",
    "# https://github.com/catboost/tutorials/blob/master/classification/classification_with_parameter_tuning_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UciAdultClassifierObjective(object):\n",
    "    def __init__(self, dataset, const_params, fold_count):\n",
    "        self._dataset = dataset\n",
    "        self._const_params = const_params.copy()\n",
    "        self._fold_count = fold_count\n",
    "        self._evaluated_count = 0\n",
    "        \n",
    "    def _to_catboost_params(self, hyper_params):\n",
    "        return {\n",
    "            'learning_rate': hyper_params['learning_rate'],\n",
    "            'depth': hyper_params['depth'],\n",
    "            'l2_leaf_reg': hyper_params['l2_leaf_reg']}\n",
    "    \n",
    "    # hyperopt optimizes an objective using `__call__` method (e.g. by doing \n",
    "    # `foo(hyper_params)`), so we provide one\n",
    "    def __call__(self, hyper_params):\n",
    "        # join hyper-parameters provided by hyperopt with hyper-parameters \n",
    "        # provided by the user\n",
    "        params = self._to_catboost_params(hyper_params)\n",
    "        params.update(self._const_params)\n",
    "        \n",
    "        print('evaluating params={}'.format(params), file=sys.stdout)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # we use cross-validation for objective evaluation, to avoid overfitting\n",
    "        scores = cgb.cv(\n",
    "            pool=self._dataset,\n",
    "            params=params,\n",
    "            fold_count=self._fold_count,\n",
    "            partition_random_seed=20181224,\n",
    "            verbose=False)\n",
    "        \n",
    "        # scores returns a dictionary with mean and std (per-fold) of metric \n",
    "        # value for each cv iteration, we choose minimal value of objective \n",
    "        # mean (though it will be better to choose minimal value among all folds)\n",
    "        # because noise is additive\n",
    "        max_mean_auc = np.max(scores['test-AUC-mean'])\n",
    "        print('evaluated score={}'.format(max_mean_auc), file=sys.stdout)\n",
    "        \n",
    "        self._evaluated_count += 1\n",
    "        print('evaluated {} times'.format(self._evaluated_count), file=sys.stdout)\n",
    "           \n",
    "        # negate because hyperopt minimizes the objective\n",
    "        return {'loss': -max_mean_auc, 'status': hyperopt.STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyper_params(dataset, const_params, max_evals=100):    \n",
    "    # we are going to optimize these three parameters\n",
    "    parameter_space = {\n",
    "        'learning_rate': hyperopt.hp.uniform('learning_rate', 0.2, 1.0),\n",
    "        'depth': hyperopt.hp.randint('depth', 7),\n",
    "        'l2_leaf_reg': hyperopt.hp.uniform('l2_leaf_reg', 1, 10)}\n",
    "    objective = UciAdultClassifierObjective(dataset=dataset, const_params=const_params, fold_count=6)\n",
    "    trials = hyperopt.Trials()\n",
    "    best = hyperopt.fmin(\n",
    "        fn=objective,\n",
    "        space=parameter_space,\n",
    "        algo=hyperopt.rand.suggest,\n",
    "        max_evals=max_evals,\n",
    "        rstate=np.random.RandomState(seed=20181224))\n",
    "    return best\n",
    "\n",
    "def train_best_model(X, y, const_params, max_evals=100, use_default=False):\n",
    "    # convert pandas.DataFrame to catboost.Pool to avoid converting it on each \n",
    "    # iteration of hyper-parameters optimization\n",
    "    dataset = cgb.Pool(X, y, cat_features=np.where(X.dtypes != np.float)[0])\n",
    "    \n",
    "    if use_default:\n",
    "        # pretrained optimal parameters\n",
    "        best = {\n",
    "            'learning_rate': 0.4234185321620083, \n",
    "            'depth': 5, \n",
    "            'l2_leaf_reg': 9.464266235679002}\n",
    "    else:\n",
    "        best = find_best_hyper_params(dataset, const_params, max_evals=max_evals)\n",
    "    \n",
    "    # merge subset of hyper-parameters provided by hyperopt with hyper-parameters \n",
    "    # provided by the user\n",
    "    hyper_params = best.copy()\n",
    "    hyper_params.update(const_params)\n",
    "    \n",
    "    # drop `use_best_model` because we are going to use entire dataset for \n",
    "    # training of the final model\n",
    "    hyper_params.pop('use_best_model', None)\n",
    "    \n",
    "    model = cgb.CatBoostClassifier(**hyper_params)\n",
    "    model.fit(dataset, verbose=False)\n",
    "    \n",
    "    return model, hyper_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params are {'learning_rate': 0.4234185321620083, 'depth': 5, 'l2_leaf_reg': 9.464266235679002, 'task_type': 'CPU', 'loss_function': 'Logloss', 'eval_metric': 'AUC', 'custom_metric': ['AUC'], 'iterations': 100, 'random_seed': 20181224}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make it True if your want to use GPU for training\n",
    "have_gpu = False\n",
    "# skip hyper-parameter optimization and just use provided optimal parameters\n",
    "use_optimal_pretrained_params = True\n",
    "# number of iterations of hyper-parameter search\n",
    "hyperopt_iterations = 30\n",
    "const_params = dict({\n",
    "    'task_type': 'GPU' if have_gpu else 'CPU',\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC', \n",
    "    'custom_metric': ['AUC'],\n",
    "    'iterations': 100,\n",
    "    'random_seed': 20181224})\n",
    "\n",
    "model, params = train_best_model(\n",
    "    X_train, y_train, \n",
    "    const_params, \n",
    "    max_evals=hyperopt_iterations, \n",
    "    use_default=use_optimal_pretrained_params)\n",
    "print('best params are {}'.format(params), file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.3791208791208791\n",
      "Recall:  0.518796992481203\n",
      "Prec.:  0.2987012987012987\n",
      "Acc.:  0.8043290043290043\n"
     ]
    }
   ],
   "source": [
    "# predict on X_test and classify based on a threshold of 0.6\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred[y_pred >= 0.6] = 1\n",
    "y_pred[y_pred < 0.6] = 0\n",
    "\n",
    "print (\"F1: \", f1_score(y_pred, y_test))\n",
    "print (\"Recall: \", recall_score(y_pred, y_test))\n",
    "print (\"Prec.: \", precision_score(y_pred, y_test))\n",
    "print (\"Acc.: \", accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81151465, 0.18848535],\n",
       "       [0.9456155 , 0.0543845 ],\n",
       "       [0.91688247, 0.08311753],\n",
       "       ...,\n",
       "       [0.95796661, 0.04203339],\n",
       "       [0.89550243, 0.10449757],\n",
       "       [0.41967292, 0.58032708]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now import test set without labels and make predictions\n",
    "test = pd.read_csv('HW4 - test data.csv',header=0)\n",
    "\n",
    "y_pred = model.predict_proba(test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classes_\n",
    "# predictions we want will be in the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final predictions for ensembling later in Excel\n",
    "np.savetxt(\"catboost.csv\", y_pred , delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this tutorial as a resource for tuning my parameters for XGBoost using Bayesian Optimization:\n",
    "# https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define R-Squared\n",
    "def xgb_r2(preds, X_train):\n",
    "    labels = X_train.get_label()\n",
    "    return 'r2', r2_score(preds, labels)\n",
    "  \n",
    "X_train = xgb.DMatrix(X, y, feature_names=X.columns.values)\n",
    "\n",
    "# define objective function\n",
    "def hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma ):\n",
    "    params = {\n",
    "    'n_estimators': 300,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric':'mae', # Optional --> Use eval_metric if you want to stop evaluation based on eval_metric \n",
    "    'silent': 1\n",
    "     }\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    scores = xgb.cv(params, X_train, num_boost_round=1000,verbose_eval=False, early_stopping_rounds=10, feval=xgb_r2, maximize=True, nfold=5)\n",
    "    return  scores['test-r2-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters to search through\n",
    "pds ={\n",
    "  'min_child_weight':(14, 20),\n",
    "  'gamma':(0, 5),\n",
    "  'subsample':(0.5, 1),\n",
    "  'colsample_bytree':(0.1, 1),\n",
    "  'max_depth': (5, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "|  1        | -1.8      |  0.7942   |  0.1038   |  8.168    |  18.49    |  0.7493   |\n",
      "|  2        | -2.602    |  0.3023   |  0.9903   |  8.803    |  15.01    |  0.5442   |\n",
      "|  3        | -5.947    |  0.7168   |  4.767    |  5.02     |  17.07    |  0.9063   |\n",
      "|  4        | -4.898    |  0.6513   |  3.609    |  6.459    |  19.51    |  0.8573   |\n",
      "|  5        | -2.262    |  0.5883   |  0.7109   |  6.867    |  18.04    |  0.7209   |\n",
      "|  6        | -2.189    |  0.1      |  0.0      |  10.0     |  17.44    |  1.0      |\n",
      "|  7        | -2.07     |  0.1      |  0.0      |  10.0     |  20.0     |  0.5      |\n",
      "|  8        | -2.155    |  0.1      |  0.0      |  6.743    |  20.0     |  0.5      |\n",
      "|  9        | -1.94     |  1.0      |  0.0      |  5.0      |  14.0     |  0.5      |\n",
      "|  10       | -1.75     |  1.0      |  0.0      |  6.752    |  15.86    |  1.0      |\n",
      "|  11       | -2.292    |  0.1      |  0.0      |  5.0      |  16.33    |  0.5      |\n",
      "|  12       | -2.227    |  0.1      |  0.0      |  6.791    |  14.0     |  1.0      |\n",
      "|  13       | -1.776    |  1.0      |  0.0      |  10.0     |  14.0     |  1.0      |\n",
      "|  14       | -1.743    |  1.0      |  0.0      |  8.47     |  20.0     |  1.0      |\n",
      "|  15       | -1.825    |  1.0      |  0.0      |  5.0      |  18.88    |  1.0      |\n",
      "|  16       | -1.962    |  1.0      |  0.0      |  6.575    |  17.91    |  0.5      |\n",
      "|  17       | -1.767    |  1.0      |  0.0      |  8.648    |  16.31    |  1.0      |\n",
      "|  18       | -1.751    |  1.0      |  0.0      |  10.0     |  19.01    |  1.0      |\n",
      "|  19       | -1.776    |  1.0      |  0.0      |  8.198    |  14.0     |  1.0      |\n",
      "|  20       | -1.752    |  1.0      |  0.0      |  7.611    |  17.83    |  1.0      |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# define Bayesian Optimization function and maximize the output of objective function\n",
    "# here, the sum of init_points and n_iter will equal the total number of rounds\n",
    "optimizer = BayesianOptimization(hyp_xgb, pds, random_state=10)\n",
    "optimizer.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1.0,\n",
       " 'gamma': 0.0,\n",
       " 'max_depth': 8.470384396725334,\n",
       " 'min_child_weight': 20.0,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get optimized parameters\n",
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train instances: 5390\n",
      "Number of test instances: 2310\n"
     ]
    }
   ],
   "source": [
    "# redefine X_train, X_test, y_train, and y_test since we've used them previously\n",
    "# split into X and y\n",
    "X = sample_data.iloc[:, data.columns != 'adopter']\n",
    "y = sample_data.iloc[:, data.columns == 'adopter']\n",
    "\n",
    "# split dataset for cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print (\"Number of train instances: {}\".format(len(X_train)))\n",
    "print (\"Number of test instances: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.20217\ttrain-error:0.18126\n",
      "[1]\ttest-error:0.19870\ttrain-error:0.17681\n",
      "[2]\ttest-error:0.19913\ttrain-error:0.17403\n",
      "[3]\ttest-error:0.19827\ttrain-error:0.17180\n",
      "[4]\ttest-error:0.19351\ttrain-error:0.16865\n",
      "[5]\ttest-error:0.19524\ttrain-error:0.17069\n",
      "[6]\ttest-error:0.19784\ttrain-error:0.16883\n",
      "[7]\ttest-error:0.19567\ttrain-error:0.16846\n",
      "[8]\ttest-error:0.19654\ttrain-error:0.16883\n",
      "[9]\ttest-error:0.20000\ttrain-error:0.16475\n",
      "[10]\ttest-error:0.20043\ttrain-error:0.16289\n",
      "[11]\ttest-error:0.19437\ttrain-error:0.16215\n",
      "[12]\ttest-error:0.19394\ttrain-error:0.16197\n",
      "[13]\ttest-error:0.19178\ttrain-error:0.16197\n",
      "[14]\ttest-error:0.19394\ttrain-error:0.16085\n",
      "[15]\ttest-error:0.19351\ttrain-error:0.15993\n",
      "[16]\ttest-error:0.19610\ttrain-error:0.15733\n",
      "[17]\ttest-error:0.19610\ttrain-error:0.15788\n",
      "[18]\ttest-error:0.19481\ttrain-error:0.15677\n",
      "[19]\ttest-error:0.19264\ttrain-error:0.15547\n",
      "[20]\ttest-error:0.19307\ttrain-error:0.15603\n",
      "[21]\ttest-error:0.19610\ttrain-error:0.15621\n",
      "[22]\ttest-error:0.19221\ttrain-error:0.15454\n",
      "[23]\ttest-error:0.19307\ttrain-error:0.15343\n",
      "[24]\ttest-error:0.19351\ttrain-error:0.15213\n",
      "[25]\ttest-error:0.19524\ttrain-error:0.15269\n",
      "[26]\ttest-error:0.19481\ttrain-error:0.15213\n",
      "[27]\ttest-error:0.19394\ttrain-error:0.14991\n",
      "[28]\ttest-error:0.19524\ttrain-error:0.14898\n",
      "[29]\ttest-error:0.19264\ttrain-error:0.14916\n",
      "[30]\ttest-error:0.19307\ttrain-error:0.14768\n",
      "[31]\ttest-error:0.19567\ttrain-error:0.14768\n",
      "[32]\ttest-error:0.19567\ttrain-error:0.14842\n",
      "[33]\ttest-error:0.19437\ttrain-error:0.14805\n",
      "[34]\ttest-error:0.19654\ttrain-error:0.14638\n",
      "[35]\ttest-error:0.19567\ttrain-error:0.14564\n",
      "[36]\ttest-error:0.19481\ttrain-error:0.14490\n",
      "[37]\ttest-error:0.19610\ttrain-error:0.14490\n",
      "[38]\ttest-error:0.19567\ttrain-error:0.14378\n",
      "[39]\ttest-error:0.19654\ttrain-error:0.14267\n",
      "[40]\ttest-error:0.19827\ttrain-error:0.14267\n",
      "[41]\ttest-error:0.19740\ttrain-error:0.14119\n",
      "[42]\ttest-error:0.19610\ttrain-error:0.13840\n",
      "[43]\ttest-error:0.19610\ttrain-error:0.13655\n",
      "[44]\ttest-error:0.19524\ttrain-error:0.13599\n",
      "[45]\ttest-error:0.19481\ttrain-error:0.13525\n",
      "[46]\ttest-error:0.19697\ttrain-error:0.13506\n",
      "[47]\ttest-error:0.19654\ttrain-error:0.13432\n",
      "[48]\ttest-error:0.19481\ttrain-error:0.13395\n",
      "[49]\ttest-error:0.19307\ttrain-error:0.13395\n",
      "[50]\ttest-error:0.19610\ttrain-error:0.13284\n",
      "[51]\ttest-error:0.19740\ttrain-error:0.13302\n",
      "[52]\ttest-error:0.19697\ttrain-error:0.13377\n",
      "[53]\ttest-error:0.19654\ttrain-error:0.13395\n",
      "[54]\ttest-error:0.19437\ttrain-error:0.13210\n",
      "[55]\ttest-error:0.19567\ttrain-error:0.13117\n",
      "[56]\ttest-error:0.19481\ttrain-error:0.12950\n",
      "[57]\ttest-error:0.19481\ttrain-error:0.13080\n",
      "[58]\ttest-error:0.19481\ttrain-error:0.12987\n",
      "[59]\ttest-error:0.19437\ttrain-error:0.13006\n",
      "[60]\ttest-error:0.19394\ttrain-error:0.12950\n",
      "[61]\ttest-error:0.19437\ttrain-error:0.12913\n",
      "[62]\ttest-error:0.19394\ttrain-error:0.12857\n",
      "[63]\ttest-error:0.19437\ttrain-error:0.12839\n",
      "[64]\ttest-error:0.19351\ttrain-error:0.12783\n",
      "[65]\ttest-error:0.19437\ttrain-error:0.12727\n",
      "[66]\ttest-error:0.19524\ttrain-error:0.12542\n",
      "[67]\ttest-error:0.19654\ttrain-error:0.12597\n",
      "[68]\ttest-error:0.19740\ttrain-error:0.12523\n",
      "[69]\ttest-error:0.19610\ttrain-error:0.12505\n",
      "[70]\ttest-error:0.19654\ttrain-error:0.12505\n",
      "[71]\ttest-error:0.19610\ttrain-error:0.12430\n",
      "[72]\ttest-error:0.19524\ttrain-error:0.12375\n",
      "[73]\ttest-error:0.19654\ttrain-error:0.12356\n",
      "[74]\ttest-error:0.19610\ttrain-error:0.12263\n",
      "[75]\ttest-error:0.19610\ttrain-error:0.12134\n",
      "[76]\ttest-error:0.19610\ttrain-error:0.12078\n",
      "[77]\ttest-error:0.19740\ttrain-error:0.12041\n",
      "[78]\ttest-error:0.19697\ttrain-error:0.12059\n",
      "[79]\ttest-error:0.19654\ttrain-error:0.12022\n",
      "[80]\ttest-error:0.19827\ttrain-error:0.11967\n",
      "[81]\ttest-error:0.19827\ttrain-error:0.11892\n",
      "[82]\ttest-error:0.19870\ttrain-error:0.11929\n",
      "[83]\ttest-error:0.19654\ttrain-error:0.11929\n",
      "[84]\ttest-error:0.19610\ttrain-error:0.11725\n",
      "[85]\ttest-error:0.19654\ttrain-error:0.11670\n",
      "[86]\ttest-error:0.19740\ttrain-error:0.11466\n",
      "[87]\ttest-error:0.19827\ttrain-error:0.11466\n",
      "[88]\ttest-error:0.19827\ttrain-error:0.11410\n",
      "[89]\ttest-error:0.19784\ttrain-error:0.11299\n",
      "[90]\ttest-error:0.19827\ttrain-error:0.11206\n",
      "[91]\ttest-error:0.19740\ttrain-error:0.11150\n",
      "[92]\ttest-error:0.19697\ttrain-error:0.11187\n",
      "[93]\ttest-error:0.19697\ttrain-error:0.11132\n",
      "[94]\ttest-error:0.19913\ttrain-error:0.11095\n",
      "[95]\ttest-error:0.19654\ttrain-error:0.11058\n",
      "[96]\ttest-error:0.19654\ttrain-error:0.11076\n",
      "[97]\ttest-error:0.19654\ttrain-error:0.11020\n",
      "[98]\ttest-error:0.19610\ttrain-error:0.10891\n",
      "[99]\ttest-error:0.19610\ttrain-error:0.10891\n"
     ]
    }
   ],
   "source": [
    "# create train and test xgb matrices\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "num_rounds = 100\n",
    "\n",
    "params = {\n",
    "    'max_depth':7,\n",
    "    'n_estimators': 5000,\n",
    "    'gamma': 0,\n",
    "    'scale_pos_weight': 1,\n",
    "    'eta': 0.1,\n",
    "    'reg_alpha': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 557,\n",
    "    'colsample_bytree': 1,\n",
    "    'min_child_weight': 17.90432554809831,\n",
    "    'subsample': 1\n",
    "}\n",
    "\n",
    "test_train_split = [(dtest, 'test'), (dtrain, 'train')]\n",
    "\n",
    "boost = xgb.train(params,\n",
    "                 dtrain,\n",
    "                 num_rounds, \n",
    "                 test_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069264069264069\n",
      "0.28525641025641024\n",
      "0.5493827160493827\n",
      "0.19264069264069264\n"
     ]
    }
   ],
   "source": [
    "# predict on dtest and classify based on a threshold of 0.6\n",
    "y_pred = boost.predict(dtest)\n",
    "y_pred[y_pred >= 0.6] = 1\n",
    "y_pred[y_pred < 0.6] = 0\n",
    "\n",
    "print (accuracy_score(y_pred, y_test))\n",
    "print (f1_score(y_pred, y_test))\n",
    "print (recall_score(y_pred, y_test))\n",
    "print (precision_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction(adopter)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.129029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.025890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.114438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.035663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86676</td>\n",
       "      <td>0.015777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86677</td>\n",
       "      <td>0.555487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86678</td>\n",
       "      <td>0.102279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86679</td>\n",
       "      <td>0.139987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86680</td>\n",
       "      <td>0.727655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86681 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction(adopter)\n",
       "0                 0.129029\n",
       "1                 0.025890\n",
       "2                 0.114438\n",
       "3                 0.016514\n",
       "4                 0.035663\n",
       "...                    ...\n",
       "86676             0.015777\n",
       "86677             0.555487\n",
       "86678             0.102279\n",
       "86679             0.139987\n",
       "86680             0.727655\n",
       "\n",
       "[86681 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now import test set without labels and make predictions\n",
    "test = pd.read_csv('HW4 - test data.csv',header=0)\n",
    "test = xgb.DMatrix(test)\n",
    "\n",
    "y_pred = boost.predict(test)\n",
    "\n",
    "y_pred = pd.DataFrame({'prediction(adopter)': y_pred })\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final predictions for ensembling later in Excel\n",
    "np.savetxt(\"xgboost.csv\", y_pred , delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I originally did each model in separate Python notebooks, so that's why I export to Excel to combine them\n",
    "# I averaged the predictions in Excel, then classified each one using a threshold of 0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
